{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "active-stranger",
   "metadata": {},
   "source": [
    "# Big Data Concepts in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-edward",
   "metadata": {},
   "source": [
    "### Lambda Functions\n",
    "* Defined inline  and are limited to a single expression\n",
    "1. Takes an iterable\n",
    "2. Sets the iterable to lowercase\n",
    "3. Sorts the items within the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "voluntary-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'dank', 'is', 'programming']\n",
      "['dank', 'is', 'programming', 'Python']\n"
     ]
    }
   ],
   "source": [
    "x = ['Python', 'programming', 'is', 'dank']\n",
    "print(sorted(x))\n",
    "print(sorted(x, key=lambda arg: arg.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-illinois",
   "metadata": {},
   "source": [
    "#### Filter( )\n",
    "\n",
    "* Function: Filters out items based on a condition typically as a lambda function\n",
    "1. Takes an iterable\n",
    "2. Calls the lambda function on each item\n",
    "3. Returns the items where lambda == True\n",
    "\n",
    "* Filter in Method 1 returns an iterable rather than the actual item\n",
    "    * Useful for Big Data sets as this prevents us from having to store datasets up to terabytes in size in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "diverse-sender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'dank']\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "print(list(filter(lambda arg: len(arg) < 8, x)))\n",
    "\n",
    "# Method 2\n",
    "def is_less_than_8_characters(item):\n",
    "    return len(item) < 8\n",
    "\n",
    "x = ['Python', 'programming', 'is', 'dank!']\n",
    "results = []\n",
    "\n",
    "for item in x:\n",
    "    if is_less_than_8_characters(item):\n",
    "        results.append(item)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-judge",
   "metadata": {},
   "source": [
    "#### Map( )\n",
    "\n",
    "* Function: Applies a 1:1 mapping of the original items to a function return\n",
    "\n",
    "1. Takes an iterable\n",
    "2. Calls the lambda function on each item\n",
    "3. Returns the mapped item output\n",
    "\n",
    "* Note: The Map() function (as opposed to filter()) will always return the same number of items passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abstract-corpus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PYTHON', 'PROGRAMMING', 'IS', 'DANK']\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "print(list(map(lambda arg: arg.upper(), x)))\n",
    "\n",
    "# Method 2:\n",
    "results = []\n",
    "\n",
    "x = ['Python', 'programming', 'is', 'dank!']\n",
    "for item in x:\n",
    "    results.append(item.upper())\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-scanner",
   "metadata": {},
   "source": [
    "#### Reduce ( ) \n",
    "* Function: Applies a function to elements of an iterable to transform them into a single value\n",
    "\n",
    "1. Takes an iterable\n",
    "2. Calls the iterable and its subsequent element\n",
    "3. Returns the combined vlaue of the iterable and its following element\n",
    "\n",
    "* Note: In this function, the items in teh iterable from left to right are combined into a singl item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "x = ['Python', 'programming', 'is', 'awesome!']\n",
    "print(reduce(lambda val1, val2: val1 + val2, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-savings",
   "metadata": {},
   "source": [
    "### Spark and PySpark\n",
    "\n",
    "#### What is Spark?\n",
    "* Apache Spark can be considered as a generic engine for processing large amounts of data\n",
    "* Primarily runs on Scala and JVM\n",
    "\n",
    "#### What is PySpark?\n",
    "* Python-based wrapper on top of the Scala API\n",
    "    * Like a library that allows for the processing of large amounts of data on a single machine / cluster\n",
    "    * Almost like utilizing the multithreading / multiprocessing without those modules\n",
    "* Pyspark can exist due to the following: \n",
    "    * Scala is functional-based\n",
    "    * Functional code is easier to parallelize\n",
    "\n",
    "#### Pyspark API and Data Structures\n",
    "* RDDs: ***R***esilient ***D***istributed ***D***atasets\n",
    "    * Specilized data structures to use within Spark\n",
    "    * Can almost be considered like a pandas df\n",
    "    * Hides the complexity of transforming and distributing data across multiple nodes via a scheduler\n",
    "* SparkContext:\n",
    "    * Entrypoint of any PySpark program that connects to a Spark cluster and creates RDDs\n",
    "* RDDs can be created out of common datastructures like lists and tuples\n",
    "    * Done via the parallelize( ) function\n",
    "    * As the data isn't actually stored, functions like take ( ) allows for you to see the data without destroying your machine!\n",
    "    \n",
    "``` \n",
    "      ------<------------ Worker Node: {Executor: [task, task], Cache: []}\n",
    "      |                 /\n",
    "Spark Context --> Cluster Manager \n",
    "      |                 \\\n",
    "      -------<----------- Worker Node: {Executor: [task, task], Cache: []}\n",
    "\n",
    "```\n",
    "\n",
    "##### Example\n",
    "```python\n",
    "big_list = range(10000)\n",
    "rdd = sc.parallelize(big_list, 2)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "odds.take(5)\n",
    "# [1,3,7,5,9]\n",
    "```\n",
    "\n",
    "#### MapReduce\n",
    "\n",
    "* Map: Filter and sort Data\n",
    "* Reduce: Aggregate inputs and reduce its size\n",
    "\n",
    "```\n",
    "            |Block 1 <-> Map|-\n",
    "         /                    \\               Shuffle\n",
    "Input ----> |Block 2 <-> Map|----> Combine ->    &    -- > Reduce -> Output\n",
    "         \\                    /                Sort\n",
    "            |Block 3 <-> Map|-\n",
    "```\n",
    "\n",
    "\n",
    "#### Spark vs. MapReduce\n",
    "\n",
    "* Spark Pros:\n",
    "  * Spark is great for when you can hold all of your data in memory\n",
    "      * It leverages RAM in order to be ~100x faster than MapReduce which uses diskspace\n",
    "  * Spark has a diverse set of API access to make it easy to access\n",
    "      * Python, Scala, Java, SQL\n",
    "      * MapReduce does have HIVE and PIG which allows for more access than just pure Java\n",
    "* MapReduce Pros:\n",
    "  * MapReduce is great for data that can't fit in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-edmonton",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
