{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "under-cruise",
   "metadata": {},
   "source": [
    "# Big Data Concepts in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-february",
   "metadata": {},
   "source": [
    "### Lambda Functions\n",
    "* Defined inline  and are limited to a single expression\n",
    "1. Takes an iterable\n",
    "2. Sets the iterable to lowercase\n",
    "3. Sorts the items within the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "official-portland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by Topic: [('English', 88), ('Maths', 97), ('Science', 90), ('Social sciences', 82)]\n",
      "Sorted by Score: [('Social sciences', 82), ('English', 88), ('Science', 90), ('Maths', 97)]\n"
     ]
    }
   ],
   "source": [
    "x = [('Science', 90), ('English', 88), ('Maths', 97), ('Social sciences', 82)]\n",
    "print('Sorted by Topic: %s' % sorted(x, key=lambda x: x[0]))\n",
    "print('Sorted by Score: %s' % sorted(x, key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-drama",
   "metadata": {},
   "source": [
    "#### Filter( )\n",
    "\n",
    "* Function: Filters out items based on a condition typically as a lambda function\n",
    "1. Takes an iterable\n",
    "2. Calls the lambda function on each item\n",
    "3. Returns the items where lambda == True\n",
    "\n",
    "* Filter in Method 1 returns an iterable rather than the actual item\n",
    "    * Useful for Big Data sets as this prevents us from having to store datasets up to terabytes in size in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compressed-transfer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python', 'is', 'dank!']\n",
      "['Python', 'is', 'dank!']\n"
     ]
    }
   ],
   "source": [
    "x = ['Python', 'programming', 'is', 'dank!']\n",
    "\n",
    "# Method 1\n",
    "print(list(filter(lambda arg: len(arg) < 8, x)))\n",
    "\n",
    "# Method 2\n",
    "def is_less_than_8_characters(item):\n",
    "    return len(item) < 8\n",
    "\n",
    "results = []\n",
    "for item in x:\n",
    "    if is_less_than_8_characters(item):\n",
    "        results.append(item)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-exception",
   "metadata": {},
   "source": [
    "#### Map( )\n",
    "\n",
    "* Function: Applies a 1:1 mapping of the original items to a function return\n",
    "\n",
    "1. Takes an iterable\n",
    "2. Calls the lambda function on each item\n",
    "3. Returns the mapped item output\n",
    "\n",
    "* Note: The Map() function (as opposed to filter()) will always return the same number of items passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "psychological-screw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PYTHON', 'PROGRAMMING', 'IS', 'DANK']\n"
     ]
    }
   ],
   "source": [
    "# Method 1\n",
    "print(list(map(lambda arg: arg.upper(), x)))\n",
    "\n",
    "# Method 2:\n",
    "results = []\n",
    "\n",
    "x = ['Python', 'programming', 'is', 'dank!']\n",
    "for item in x:\n",
    "    results.append(item.upper())\n",
    "    \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-adventure",
   "metadata": {},
   "source": [
    "#### Reduce ( ) \n",
    "* Function: Applies a function to elements of an iterable to transform them into a single value\n",
    "\n",
    "1. Takes an iterable\n",
    "2. Calls the iterable and its subsequent element\n",
    "3. Returns the combined vlaue of the iterable and its following element\n",
    "\n",
    "* Note: In this function, the items in teh iterable from left to right are combined into a singl item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-scholar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "x = ['Python', 'programming', 'is', 'awesome!']\n",
    "print(reduce(lambda val1, val2: val1 + val2, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-faith",
   "metadata": {},
   "source": [
    "---\n",
    "### Spark and PySpark\n",
    "\n",
    "#### What is Spark?\n",
    "* Apache Spark can be considered as a generic engine for processing large amounts of data\n",
    "* Primarily runs on Scala and JVM\n",
    "\n",
    "#### What is PySpark?\n",
    "* Python-based wrapper on top of the Scala API\n",
    "    * Like a library that allows for the processing of large amounts of data on a single machine / cluster\n",
    "    * Almost like utilizing the multithreading / multiprocessing without those modules\n",
    "* Pyspark can exist due to the following: \n",
    "    * Scala is functional-based\n",
    "    * Functional code is easier to parallelize\n",
    "\n",
    "#### Pyspark API and Data Structures\n",
    "* RDDs: ***R***esilient ***D***istributed ***D***atasets\n",
    "    * Specilized data structures to use within Spark\n",
    "    * Can almost be considered like a pandas df\n",
    "    * Hides the complexity of transforming and distributing data across multiple nodes via a scheduler\n",
    "    * RDD's are immutable\n",
    "        * Once created, it cannot be changed\n",
    "    * RDD's are fault tolerant\n",
    "        * Upon failure, the RDD will recover automatically\n",
    "    * Types of Operations:\n",
    "        * Transformation\n",
    "            * When applied on an RDD, these create a new RDD\n",
    "            * EX: Filter, groupBy, map\n",
    "        * Action\n",
    "            * Instructs Spark to perform a computation and send the result back to the driver \n",
    "        \n",
    "* SparkContext:\n",
    "    * Entrypoint of any PySpark program that connects to a Spark cluster and creates RDDs\n",
    "* RDDs can be created out of common datastructures like lists and tuples\n",
    "    * Done via the parallelize( ) function\n",
    "    * As the data isn't actually stored, functions like take ( ) allows for you to see the data without destroying your machine!\n",
    "    \n",
    "``` \n",
    "      ------<------------ Worker Node: {Executor: [task, task], Cache: []}\n",
    "      |                 /\n",
    "Spark Context --> Cluster Manager \n",
    "      |                 \\\n",
    "      -------<----------- Worker Node: {Executor: [task, task], Cache: []}\n",
    "\n",
    "```\n",
    "\n",
    "##### Example\n",
    "```python\n",
    "big_list = range(10000)\n",
    "rdd = sc.parallelize(big_list, 2)\n",
    "odds = rdd.filter(lambda x: x % 2 != 0)\n",
    "odds.take(5)\n",
    "# [1,3,7,5,9]\n",
    "```\n",
    "\n",
    "#### MapReduce\n",
    "\n",
    "* Map: Filter and sort data\n",
    "* Reduce: Aggregate inputs and reduce its size\n",
    "\n",
    "```\n",
    "            |Block 1 <-> Map|-\n",
    "         /                    \\               Shuffle\n",
    "Input ----> |Block 2 <-> Map|----> Combine ->    &    -- > Reduce -> Output\n",
    "         \\                    /                Sort\n",
    "            |Block 3 <-> Map|-\n",
    "```\n",
    "\n",
    "\n",
    "#### Spark vs. MapReduce\n",
    "\n",
    "* Spark Pros:\n",
    "  * Spark is great for when you can hold all of your data in memory\n",
    "      * It leverages RAM in order to be ~100x faster than MapReduce which uses diskspace\n",
    "  * Spark has a diverse set of API access to make it easy to access\n",
    "      * Python, Scala, Java, SQL\n",
    "      * MapReduce does have HIVE and PIG which allows for more access than just pure Java\n",
    "* MapReduce Pros:\n",
    "  * MapReduce is great for data that can't fit in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-damage",
   "metadata": {},
   "source": [
    "---\n",
    "### Pyspark Code Examples\n",
    "\n",
    "#### File Read In Example\n",
    "\n",
    "* Example program to count the number of A's and B's in a file\n",
    "```python\n",
    "ex_file = \"file:///home/hadoop/spark-2.1.0-bin/README.md\n",
    "sc = SparkContext(\"local\", \"first app\") # Define and instantiate Spark file\n",
    "logData = sc.textFile(logFile).cache() # Read the file into the cache\n",
    "numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "print('Lines with a: %i, lines with b: %i' % (numAs, numBs)\n",
    "```\n",
    "\n",
    "### RDD Operations\n",
    "\n",
    "#### count()\n",
    "* Returns the number of elements in the RDD\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'count app')\n",
    "words = sc.parallelize (['scala', 'java', 'hadoop', 'spark'])\n",
    "counts = words.count()\n",
    "print('Number of counts: %i (counts))\n",
    "# > Number of counts: 4\n",
    "```\n",
    "\n",
    "#### collect()\n",
    "* Returns all of the elements in an RDD\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'collect app')\n",
    "words = sc.parallelize (['scala', 'java', 'hadoop', 'spark'])\n",
    "coll = words.collect()\n",
    "print('Elements: %s' % coll)\n",
    "# > Elements: ['scala', 'java', 'hadoop', 'spark']\n",
    "```\n",
    "\n",
    "#### foreach()\n",
    "* Returns only the elements which meet the condition of hte function inside the foreach\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'foreach app')\n",
    "words = sc.parallelize (['scala', 'java', 'hadoop', 'spark'])\n",
    "def f(x): print(x)\n",
    "fore = words.foreach(f)\n",
    "# > scala\n",
    "#   java\n",
    "#   hadoop\n",
    "#   spark\n",
    "```\n",
    "\n",
    "#### filter(f)\n",
    "* Returns a new RDD which contain the elements that satisfy the conditions of a filter\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'filter app')\n",
    "words = sc.parallelize (['scala', 'java', 'hadoop', 'spark'])\n",
    "words_filter = words.filter(lambda x: 's' in x) # create filter\n",
    "filtered = words.filter.collect() # gather information\n",
    "print('Filtered RDD: %s' % filtered)\n",
    "# > ['scala', 'spark']\n",
    "```\n",
    "\n",
    "#### map(f, preservesPartitioning = False)\n",
    "* Returns a new RDD filled with a mapped output for each element\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'map app')\n",
    "words = sc.parallelize (['scala', 'java', 'hadoop', 'spark'])\n",
    "words_map = words.map(lambda x: (x, x.upper)\n",
    "mapping = words_map.collect()\n",
    "print('Key Val Pair: %s' % mapping)\n",
    "# > Key Val Pair: [('scala', 'SCALA'), ('java', 'JAVA'), '(hadoop', 'HADOOP'), ('spark', 'SPARK')]\n",
    "```\n",
    "\n",
    "#### reduce(f)\n",
    "* Returns the associated RDD element with a specified binary operation\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'reduce app')\n",
    "nums = sc.parallelize ([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)\n",
    "print('Sum of all elements: %i' % adding)\n",
    "# > Sum of all elements: 15\n",
    "```\n",
    "\n",
    "#### join(other, numPartitions=None)\n",
    "* Returns an RDD that has joined matching keys \n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'join app')\n",
    "x = sc.parallelize([('spark', 1), ('hadoop', 4)])\n",
    "y = sc.parallelize([('spark', 2), ('hadoop', 5)])\n",
    "joined = x.join(y)\n",
    "final = joined.collect()\n",
    "print('Joined RDD: %s' % final)\n",
    "# > Joined RDD: [('spark', (1, 2)), ('hadoop', (4, 5))]\n",
    "```\n",
    "\n",
    "#### cache()\n",
    "* Adds the RDD to the default storage level and checks if its cached or not\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'map app')\n",
    "words = sc.parallelize (['scala', 'java', 'hadoop', 'spark'])\n",
    "words.cache()\n",
    "caching = words.persist().is_cached\n",
    "print('Cached: %s' % cachine)\n",
    "# > Cached: true\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-treat",
   "metadata": {},
   "source": [
    "### PySpark Broadcast & Accumulator\n",
    "\n",
    "* Spark will use shared variables for parallel processing\n",
    "    * These typically will get send when teh driver sends a tasks to the cluster executor\n",
    "    \n",
    "* Shared Variable Types:\n",
    "    * Broadcast\n",
    "        * Saves the copy of data across all nodes\n",
    "        * Cached on all machines and not sent on machines with tasks\n",
    "        ```python\n",
    "        from pyspark import SparkContext \n",
    "        sc = SparkContext(\"local\", \"Broadcast app\") \n",
    "        words_new = sc.parallelize (['scala', 'java', 'hadoop', 'spark'])\n",
    "        data = words_new.value \n",
    "        print \"Stored data -> %s\" % (data) \n",
    "        # > ['scala', 'java', 'hadoop', 'spark']\n",
    "        ```\n",
    "    * Accumulator\n",
    "        * Used for aggregating information through associative and commutative operations\n",
    "        * Data is used ONLY within the driver program\n",
    "        ```python\n",
    "        from pyspark import SparkContext \n",
    "        sc = SparkContext(\"local\", \"Accumulator app\") \n",
    "        num = sc.accumulator(10) \n",
    "        def f(x): \n",
    "           global num \n",
    "           num+=x \n",
    "        rdd = sc.parallelize([20,30,40,50]) \n",
    "        rdd.foreach(f) \n",
    "        final = num.value \n",
    "        print \"Accumulated value is -> %i\" % (final)\n",
    "        # > Accumulated value is -> 150\n",
    "        ```\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-logistics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
