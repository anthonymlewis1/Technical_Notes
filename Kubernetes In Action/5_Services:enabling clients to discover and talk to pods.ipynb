{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Services: enabling clients to discover and talk to pods\n",
    "\n",
    "## Kubectl Commands\n",
    "* kubectl get service: Gets and displays info on all existing services\n",
    "* kubectl exec {name} -- {bash command}: Remotely runs commands inside an existing container of a pod\n",
    "* gcloud compute firewall-rules create {rule_name} --allow=tcp:{PORT}: Allow external connections to a given node port. \n",
    "\n",
    "## Introducing Services\n",
    "* Reasons that you can't configure each client app to an exact IP address\n",
    "    * Pods are ephemeral\n",
    "    * Kubernetes assigns an IP address toa  pod after the pod has been scheduled to a node and before it's started\n",
    "    * Horizontal scalaing means multiple pods may provide the same service\n",
    "\n",
    "* Service: \n",
    "    * Resource to make a single, constant point of entry to a group of pods providing the same service\n",
    "        * Clients can open connections to that IP and port\n",
    "        * Individual pods can still get moved around the cluster at any time!\n",
    "* Creating services:\n",
    "    * Connections to a given service are load-balanced across all backing pods\n",
    "    * Label selectors are used to specify which groups belong to the same set\n",
    "    * kubectl create svc\n",
    "* Example Service Workflow:\n",
    "    * kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153\n",
    "        * kubia-7nog1 = K8s Node\n",
    "        * Service IP\n",
    "    * Curl is executed inside of a container with specified naame\n",
    "    * Curl sends HTTP GET request\n",
    "    * Service redirects HTTP connection to a randomly selected pod\n",
    "    * Pod processes request\n",
    "    * HTTP resquest is sent back to curl\n",
    "    * Output of the curl command is sent back to kubectl\n",
    "* Service Parameters\n",
    "    * sessionAffinity: ClientIP -> Redirects all requests made by a given IP to the same pod\n",
    "    * targetPort: Protocol or port number for the pod\n",
    "    * port: Port number for the service\n",
    "* Discovering Internal Services\n",
    "    * How can you get the IP and port of a service prior to creation?\n",
    "        * Kubernetes initializes some envars that can be useful to view\n",
    "            * KUBIA_SERVICE_HOST\n",
    "            * KUBIA_SERVICE_PORT\n",
    "        * DNS server lookups\n",
    "            * Domain Name Service: Stores/finds IP addresses for services\n",
    "            * Each service gets a DNS entry in the internal DNS server\n",
    "            * Client pods can access the service through its fully qualified domain name (FQDN)\n",
    "\n",
    "```\n",
    "# Example Service\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata: \n",
    "  name: external-service\n",
    "spec:\n",
    "  ports:\n",
    "  - name: http\n",
    "    port: 80\n",
    "    targetPort: http\n",
    "  - name: https\n",
    "    port: 443\n",
    "    targetPort: https\n",
    "```\n",
    "\n",
    "## Introducing Endpoints\n",
    "* Connecting to services outside the cluster\n",
    "    * Sometimes we want to have services redirect connections to external IPs\n",
    "    * Having this allows for having both service load balancing / load discovery while having external connections just like internal services\n",
    "* Endpoints:\n",
    "    * List of target IP addresses and ports exposing a service\n",
    "    * Request -> Service -> Endpoints -> Pods\n",
    "    * While endpoints are typically attached to services, they are a separate resource\n",
    "    * Endpoints need to have the same name as the service\n",
    "    * If you create a service without any target pods then you'll have to also make an endpoints resource!\n",
    "    * If you want to migrate the service to be inside of k8s, then you can add a selector to the service which allows for automatic managing!\n",
    "\n",
    "```\n",
    "# Example Endpoints\n",
    "apiVersion: v1\n",
    "kind: Endpoints\n",
    "metadata:\n",
    "  name: external-service\n",
    "subsets:\n",
    "  - addresses:\n",
    "    - ip: 11.11.11.11\n",
    "    - ip: 22.22.22.22\n",
    "    ports:\n",
    "    - port: 80\n",
    "```\n",
    "\n",
    "## Exposing services to external clients\n",
    "* Using a NodePort service\n",
    "    * Principal:\n",
    "        * Each cluster node opens a port on all of K8s nodes\n",
    "            * The same port across all of the nodes\n",
    "        * Traffic is redirected from the port to the underlying service / pods\n",
    "    * Workflow:\n",
    "        * Client puts out a request to Port 30123 on a node with a given external IP\n",
    "        * The node will redirect the traffic to the service\n",
    "        * The service will redirect the traffic to the Pod via Port 8080. \n",
    "    * Before you can open this up, you'll need to configure the firewall rules to allow for external connections to the nodes on taht port\n",
    "    * Way to get external IPs:\n",
    "        * kubectl get nodes -o jsonpath='{.items[*].status. ➥ addresses[?(@.type==\"ExternalIP\")].address}'\n",
    "    * If you don't specify a nodePort then k8s will pick one automatically\n",
    "```\n",
    "# Example NodePort\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "    name: kubia-nodeport\n",
    "spec:\n",
    "  type: NodePort\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8080\n",
    "    nodePort: 30123\n",
    "  selector:\n",
    "    app: kubia\n",
    "\n",
    "```\n",
    "* Using an external LoadBalancer\n",
    "    * Similar to NodePort but you connect via the balancer rather than the node's ip:\n",
    "        * Better in the case that a node goes down!\n",
    "    * You can get the load balancer IP by doing a simple get on the service\n",
    "    * No need to mess with firewalls here!\n",
    "    * Workflow:\n",
    "        * Client puts out a request to the load balancer with an external IP\n",
    "        * The load balancer will redirect the traffic to a node on port 32143\n",
    "        * The node will redirect the traffic to the service\n",
    "        * The service will redirect the traffic to the Pod via Port 8080. \n",
    "\n",
    "```\n",
    "# Example LoadBalancer\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: kubia-loadbalancer\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8080\n",
    "    nodePort: 32143\n",
    "  selector:\n",
    "app: kubia\n",
    "```\n",
    "* Using an Ingress resource\n",
    "    * LoadBalancers and NodePorts both require their own public IP to work\n",
    "    * Ingresses only require 1 (even when providing access to many services)\n",
    "        * The client sends a host and path in the request to show what it wants\n",
    "    * Ingresses operate at the application layer of the network stack\n",
    "    * To use an ingress object, you will need to have an ingress controller in the cluster\n",
    "    * As with LoadBalancers, the IP address is noted in the kubectl get\n",
    "    * Ingress resources also provisision a LoadBalancer behind the scenes!\n",
    "    * Workflow:\n",
    "        * Client does a DNS lookup / connects to kubia.example.com to get the ingress controller IP\n",
    "        * Client sends a GET request with the header {host: kubia.example.com} to the ingress controller\n",
    "        * The Ingress controller looks at the header and redirects the traffic to the Ingress -> Service -> Endpoints -> Pod\n",
    "\n",
    "```\n",
    "# Example Ingress\n",
    "apiVersion: extensions/v1beta1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: kubia\n",
    "spec:\n",
    "  rules:\n",
    "  - host: kubia.example.com\n",
    "    http: paths:\n",
    "      - path: /kubia\n",
    "        backend:\n",
    "          serviceName: kubia\n",
    "          servicePort: 80\n",
    "      - path: /bar\n",
    "        backend:\n",
    "          serviceName: bar\n",
    "          servicePort: 80\n",
    "```\n",
    "\n",
    "## Examining Readiness Probes\n",
    "* What happens when you spin up new pods but they aren't ready to take requests?\n",
    "    * Could need time to load configs / get data\n",
    "    * Could need time to do some warm up tasks\n",
    "* Like a liveliness probe, a readiness probe checks to see if a pod is ready\n",
    "* Types of Readiness Probes:\n",
    "    * Exec probe:\n",
    "        * A process is executed.\n",
    "        * Status is determined by exit code\n",
    "    * HTTP GET probe: \n",
    "        * Sends an HTTP GET request to the container.\n",
    "        * Status is determined by GET response code\n",
    "    * TCP Socket Probe: \n",
    "        * Opens a TCP connection to a specified port of the container\n",
    "        * Status is determined by if a connection can be established\n",
    "* If a pod fails a readiness check then it's removed from the service endpoints\n",
    "    * Unlike liveness probes, failures don't lead to pod killing / restarting\n",
    "    * If it later on passes then it's re-added back\n",
    "* A readiness probe can be defined for each container in the pod\n",
    "* In the below example, you can toggle readiness based on checking if a certain file exists or not\n",
    "* If you add a probe, you'll need to delete the old pods as they won't have the check in place\n",
    "\n",
    "```\n",
    "# Example Readiness Probe\n",
    "apiVersion: v1\n",
    "kind: ReplicationController\n",
    "...\n",
    "spec:\n",
    "  ...\n",
    "  template:\n",
    "    ... \n",
    "    spec:\n",
    "      containers:\n",
    "      - name: kubia\n",
    "        image: luksa/kubia\n",
    "        readinessProbe:\n",
    "          exec:\n",
    "            command:\n",
    "            - ls\n",
    "            - /var/ready\n",
    "\n",
    "```\n",
    "\n",
    "## Summary\n",
    "* Exposes multiple pods that match a certain label selector under a single, stable IP address and port\n",
    "* Makes services accessible from inside the cluster by default, but allows you to make the service accessible from outside the cluster by setting its type to either NodePort or LoadBalancer\n",
    "* Enables pods to discover services together with their IP addresses and ports by looking up environment variables\n",
    "* Allows discovery of and communication with services residing outside the cluster by creating a Service resource without specifying a selector, by creating an associated Endpoints resource instead\n",
    "* Provides a DNS CNAME alias for external services with the ExternalName ser- vice type\n",
    "* Exposes multiple HTTP services through a single Ingress (consuming a single IP)\n",
    "* Uses a pod container’s readiness probe to determine whether a pod should or shouldn’t be included as a service endpoint\n",
    "* Enables discovery of pod IPs through DNS when you create a headless service"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
