{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Replication and other controllers: deploying managed pods\n",
    "\n",
    "\n",
    "## Kubectl Commands\n",
    "* kubectl logs {name} -- previous: Shows the logs of the previous resource\n",
    "* kubectl label {resource} {name} {label_key}={label_value} --overwrite: Overwrites an existing label\n",
    "* kubectl edit rc {name}: Edits a rc's yaml config in the default text editor\n",
    "* kubectl scale rc {name} --replicas={number}: Rescales a replica to the desired number of replicas\n",
    "* kubectl delete rc {name}: Deletes a replication controller and its pods\n",
    "* kubectl delete rc {name} --cascade=False: Deletes only the replication controller (not the pods)\n",
    "\n",
    "## Examing Liveness Probes\n",
    "* When a pod is scheduled to a node, the node's Kubelet will run the container for as long as the pod exists\n",
    "    * If the container's main process crashes, then the Kubelet restarts the container\n",
    "    * If there are bugs in a container, then k8s will restart it automatically (self-healing)\n",
    "\n",
    "* Liveness Probes\n",
    "    * A way for K8s to check if a container is still alive\n",
    "    * Main Mechanisms:\n",
    "        * GET on the conatiner's IP to see if it returns a 2xx or a 3xx response\n",
    "        * TCP Socket connection to the specified container port to see if there's a response\n",
    "        * EXEC probe executes an arbitrary command in the container and checks for an exit code\n",
    "    * When a container is killed, a new container is created\n",
    "        * Not the same container being restarted\n",
    "    * It's pretty easy to setup parameter configurations for each probe\n",
    "    * Liveness checks should check only the internals of the app\n",
    "        * Ex: If a database goes down then a probe can keep on failing which won't fix the problem\n",
    "\n",
    "* Exit Codes are program error indicators\n",
    "    * 128 + x\n",
    "        * SIGKILL is 9 which is code 137\n",
    "        * SIGETERM is 15 which is code 143\n",
    "\n",
    "```\n",
    "# Liveness Probe Example\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: kubia-liveness\n",
    "spec:\n",
    "  containers:\n",
    "  - image: luksa/kubia-unhealthy\n",
    "    name: kubia\n",
    "    livenessProbe:\n",
    "      httpGet:\n",
    "        path: /\n",
    "        port: 8080\n",
    "        initialDelaySeconds: 15\n",
    "```\n",
    "\n",
    "## Examining ReplicationControllers\n",
    "* ReplicationController\n",
    "    * Kubernetes resource that ensures that the desired state of pods is always kept running\n",
    "        * If a pod goes missing/down, the RC will move to replace that pod\n",
    "        * If there are too many pods, the RC will remove the excess replicas\n",
    "    * RCs are meant to create/manage replices of a pod\n",
    "    * Components:\n",
    "        * Label Selector: Determines what pods are in the RC's scope\n",
    "        * Replica Count: Specifices the desired number of pods that should be running\n",
    "        * Pod Template: Used when creating new pod replicas\n",
    "    * Changing the Components:\n",
    "        * Changing the label selector makes the existing pods fall out of the RC scope\n",
    "            * So the currently-managed pods now behave like manually created pods\n",
    "        * Changing the replica count will alter the existing state of the pods\n",
    "        * Changing the pod template only affects new pods created by the RC\n",
    "            * You'll have to delete the old pods manually so that the RC can spin up new ones\n",
    "    * Benefits of using an RC:\n",
    "        * Ensures that a pod/replicas are always running\n",
    "        * Ensures that cluster nodes are always running\n",
    "        * Enables easy horizontal scaling of pods\n",
    "    * Normally you can do *kubectl delete* to delete both the RC and its pods\n",
    "        * If you want to keep the pods running, then you can set a cascade parameter\n",
    "\n",
    "```\n",
    "# Replication Controller Example\n",
    "apiVersion: v1\n",
    "kind: ReplicationController\n",
    "metadata:\n",
    "  name: kubia\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    app: kubia\n",
    "template:\n",
    "  metadata:\n",
    "    labels:\n",
    "      app: kubia\n",
    "  spec:\n",
    "    containers:\n",
    "    - name: kubia\n",
    "      image: luksa/kubia\n",
    "      ports:\n",
    "        - containerPort: 8080\n",
    "```\n",
    "\n",
    "## Examining ReplicaSets\n",
    "* Seen as the successor to ReplicationControllers\n",
    "    * RCs will be eventually deprecated\n",
    "* RSs are are almost exactly the same as RCs\n",
    "* ReplicaSets vs. ReplicationController\n",
    "    * RS allow for more expressive pod selectors\n",
    "        * RC label selectors only work on pods that include a certain label\n",
    "        * RS label selectors allow for matching based on presence/lack of a label or of the value\n",
    "        * RS label selectors can also group matching expressions together\n",
    "* Expression Types:\n",
    "  * matchLabels\n",
    "    * Similar to RC configs where it must match a given key:value\n",
    "  * matchExpressions\n",
    "    * In: Label's value must match one of the specified values\n",
    "    * Not IN: Label's value must not match any of the specified values\n",
    "    * Exists: Pods must include a label with a specified key\n",
    "    * DoesNotExist: Pods must not include a label with a specified key\n",
    "    * If there are multiple expressions, all of them must evaluate to true to match a pod\n",
    "\n",
    "```\n",
    "# ReplicaSet Example\n",
    "apiVersion: apps/v1beta2\n",
    "kind: ReplicaSet\n",
    "metadata:\n",
    "  name: kubia\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchExpressions:\n",
    "      - key: app\n",
    "        operator: In\n",
    "        values:\n",
    "          - kubia\n",
    "template:\n",
    "    metadata:\n",
    "        labels:\n",
    "        app: kubia\n",
    "    spec:\n",
    "        containers:\n",
    "        - name: kubia\n",
    "            image: luksa/kubia\n",
    "```\n",
    "\n",
    "\n",
    "## Examining DaemonSets\n",
    "* Sometimes you will want to apply a pod on every cluster node\n",
    "  * Ex: Infrastructure-related pods like log collectors and resource monitors\n",
    "* DaemonSets are similar to RCs and RSs but they aren't scattered around the cluster randomly\n",
    "* DaemonSets create as many pods as there are nodes\n",
    "* DaemonSets create pods from the pod template configured in it\n",
    "* DaemonSets can also be deployed to a subset of all nodes\n",
    "\n",
    "```\n",
    "# DaemonSet Example\n",
    "apiVersion: apps/v1beta2\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: ssd-monitor\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ssd-monitor\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ssd-monitor\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        disk: ssd\n",
    "      containers:\n",
    "      - name: main\n",
    "        image: luksa/ssd-monitor\n",
    "```\n",
    "\n",
    "## Examining Job Resources\n",
    "* With RCs, RSs, and DCs, these all run continuously and are never considered completed.\n",
    "  * When they exit from their process, the pods are restarted\n",
    "* Job Resources:\n",
    "  * Allows for the running of pods (whose container isn't restarted) when the process running inside finishes successfully. \n",
    "  * If there are node failures, the pods on the node managed by a job can be rescheduled like with RSs\n",
    "  * Pods managed by jobs are rescheduled until they finish successfully!\n",
    "* Important Job Resources Parameters\n",
    "  * *completions*: How many times you want the Job's pod to run sequentially\n",
    "  * *parallelism*: How many pods you want to run in parallel\n",
    "  * *activeDeadlineSeconds*: Max amount of time for a job to take\n",
    "* Job Resources can also manage CronJobs\n",
    "  * Cron jobs: Jobs that need to be executed at a specific time / time interval\n",
    "\n",
    "```\n",
    "# Example Job Resource\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: batch-job-every-fifteen-minutes\n",
    "spec:\n",
    "  completions: 5\n",
    "  parallelism: 2\n",
    "  schedule: \"0,15,30,45 * * * *\"\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        metadata:\n",
    "          labels:\n",
    "            app: periodic-batch-job\n",
    "        spec:\n",
    "          restartPolicy: OnFailure\n",
    "          containers:\n",
    "          - name: main\n",
    "            image: luksa/batch-job\n",
    "\n",
    "  ```\n",
    "\n",
    "## Summary\n",
    "* You can specify a liveness probe to have Kubernetes restart your container as soon as it’s no longer healthy (where the app defines what’s considered healthy).\n",
    "* Pods shouldn’t be created directly, because they will not be re-created if they’re deleted by mistake, if the node they’re running on fails, or if they’re evicted from the node.\n",
    "* ReplicationControllers always keep the desired number of pod replicas running.\n",
    "* Scaling pods horizontally is as easy as changing the desired replica count on a ReplicationController.\n",
    "* Pods aren’t owned by the ReplicationControllers and can be moved between them if necessary.\n",
    "* A ReplicationController creates new pods from a pod template. Changing the template has no effect on existing pods.\n",
    "* ReplicationControllers should be replaced with ReplicaSets and Deployments, which provide the same functionality, but with additional powerful features.\n",
    "* ReplicationControllers and ReplicaSets schedule pods to random cluster nodes, whereas DaemonSets make sure every node runs a single instance of a pod defined in the DaemonSet.\n",
    "* Pods that perform a batch task should be created through a Kubernetes Job resource, not directly or through a ReplicationController or similar object.\n",
    "* Jobs that need to run sometime in the future can be created through CronJob resources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
