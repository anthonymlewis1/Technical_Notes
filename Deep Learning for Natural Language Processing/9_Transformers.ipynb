{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1: Introduction\n",
    "* Bidirectional Encoder Representations from Transformers (BERT)\n",
    "    * BERT aims to derive word embeddings from raw textual data by using both the left and the right context when learning vector representations for words.\n",
    "        * As opposed to word2vec which just uses a single context\n",
    "\n",
    "### 9.1.1 BERT up close: Transformers\n",
    "* Transformers: encoder-decoder models developed by Google\n",
    "* Like Word2Vec, BERT models are trained to produce a prediction based on an internal representation of contextual info.\n",
    "    * Word2Vec predicts a word given it's immediate behavior (CBOW) or a context given a certain word (Skip-gram)\n",
    "    * CBOW: Infer a missing word from its context\n",
    "    * Skip-Gram: Predict contexts from single words\n",
    "\n",
    "* Auto-encoder: When a network encodes input data into an intermediate representation and is trained to minimize reconstruction loss\n",
    "    * Input -> Hidden -> ... -> Encoded -> ... -> Hidden -> Decoded\n",
    "    * Reconstruction loss: The error the network makes when attempting to reconstruct the original input from the encoded representation\n",
    "    * This type of learning is called 'bottleneck learning' as the lower dimension data means that the network learns to eliminate noise and focus on important dimensions.\n",
    "    \n",
    "### 9.1.2 Transformer encoders\n",
    "* A transformer encoder has two internal layers:\n",
    "    * Self-Attention Layer\n",
    "    * Fully-connected feedforward network layer\n",
    "* Every sublayer has access to the incoming input vector\n",
    "    * Skip connection: A connection between two layers bypassing the intervening layers.\n",
    "* As the input x goes into each layer (1), the original input x is combined with the output of the original layer x1 and so on.\n",
    "* With the output of the original layer + the input, the data gets normalized based on the man / stdev for all the summed inputs to the neurons in that layer.\n",
    "    * Standard normalization: Z-score normalization\n",
    "* All encoder layers are equipped with attention:\n",
    "    * Pays attention to every other input\n",
    "    * Attention Heads: Derive embeddings of every separate input element in the context of surrounding input elements.\n",
    "        * Triple of 3 matrices (K,Q,V) containing separate and (initially) randomized weights.\n",
    "    * The attention weights are computed via the softmax from Chapt 8 whicere it encodes the attention i pays to j\n",
    "        * But not j -> i which is why we have a query matrix Q and a key matrix K.\n",
    "    * What if we had multiple attention heads?\n",
    "        * Different heads specialize in different attention focuses\n",
    "            * Some could focus on prepositional phrase patterns\n",
    "            * Others could focus on verb - direct object relations\n",
    "    * When we combine all of this together into one big matrix multiplied by the original vector, the created embeddings incrementally get new self-attention context.\n",
    "        * Words pay attention to themselves and each other!\n",
    "    * With multiple encoder layers stacked with their own attention heads, we get hierarchies of attention.\n",
    "* Positional Encoding is a way to encode word positions into vectors by using sine/cosine functions.\n",
    "    * Since it uses sine/cosine, the vectors can be seen as a continous alternative to discrete bits.\n",
    "\n",
    "### 9.1.3 Transformer decoders\n",
    "* The encoder goes and embeds the words + their positions into machine-readable vectors for the input layer\n",
    "* The decoder goes and attempts to generate an output sequence word by word based on the encoder representation of the input data.\n",
    "* It does help if we give it more information!\n",
    "    * The decoder does have access to the desired output symbols excluding what it should currently predict.\n",
    "    * Autoregressive: Generating a current word based on the previous words it generates.\n",
    "* The decoder needs to start at a *shifted* position compared to the encoder\n",
    "    * 1 to the right.\n",
    "    * Needs this otherwise it would be trained to copy a sequence rather than infer next words.\n",
    "* Also, in comparison to the encoder, we need to limit the self-attention to be just backwards facing\n",
    "    * Implemented via masking out words w/ a binary filter.\n",
    "\n",
    "## 9.2 BERT: Masked language modeling\n",
    "* BERT is not a transformer as it only has the encoder portion.\n",
    "* Again, it's only meant to produce vector embeddings like Word2Vec.\n",
    "* Masked language models:\n",
    "    * Primarily used to model word distribution patterns by masking out certain words and having the model predict the words 'under the masks'.\n",
    "    * Denoising autoencoding:\n",
    "        * Attemts to reconstruct a sequence of words from a corrupted 'noisy' signal.\n",
    "        * This is the sequence of words containing masks.\n",
    "    * Derived from 1950's Cloze test for assesing the mastery of a native or 2nd language alongside the readability of text.\n",
    "        * EX: Fill in the blanks in the sentence:\n",
    "            * The _ performed an echoscopy ('nurse', 'doctor')\n",
    "    * BERT works as it exploites remote and non-adjacent context for predicting a blanked out, masked word.\n",
    "\n",
    "### 9.2.1 Training BERT\n",
    "* BERT is trained on two objectives simultaneously:\n",
    "    * Predicting words hidden under masks\n",
    "    * Predicting, for an arbitrary pair of 2 sentences, whether the 2nd sentence is a natural progression of the first.\n",
    "        * Very similar to the question and answer applications from before!\n",
    "    * As such, BERT optimizes both the loss function for sentence pair prediction and the loss function for unveiling masked tokens.\n",
    "* BERT produces a contextual embedding which has embeddings for every word from the attention patternns the input evokes from the model.\n",
    "    * 2 sentences will have different embeddings for the same word (bank):\n",
    "        * I went to the bank\n",
    "        * The man went to the river bank\n",
    "    * The encoder layers re-do their computations of key-query attention values!\n",
    "\n",
    "### 9.2.2 Fine-tuning BERT\n",
    "* A BERT embedding can be fined-tuned by connecting it to a classification problem.\n",
    "    * Also called a downstream task.\n",
    "* The loss from the classification predictions will be fed back into the pre-trained BERT model.\n",
    "* All we need to do here is just add a few softmax layers with labeled data to fine-tune the model.\n",
    "* Since it's easy to increase the accuracy, these types of models are super good for sentiment-analysis. \n",
    "* Auto-encoding model:\n",
    "    * It corrupts its input data during training, and tries to find an optimal encoding to optimally reconstruct the masked input data.\n",
    "    * These typically pair with an explicit decoder which allowes for text generation.\n",
    "\n",
    "### 9.2.3 Beyond BERT\n",
    "* Primary shortcoming of BERT is its masking approach\n",
    "    * The masks make sense for the pre-training when BERT is trained to predict words under masks\n",
    "    * But it's not relevent for the fine-tuning\n",
    "* BERT also makes an independence assumption\n",
    "    * Sometimes the interactions between the masked tokens can be lost.\n",
    "    * Every mask is created in isolation between neighboring masks.\n",
    "* How can we get also the information between the masks?\n",
    "    * Enter XLNet!\n",
    "    * Example:\n",
    "        * [MASK][MASK] and a Happy New Year\n",
    "        * BERT: log(P(Merry|and a happy New year))+log(P(Christmas|and a happy New YEAR))\n",
    "        * XLNet: log(P(Merry|and a happy New year))+log(P(Christmas|Merry, and a happy Near YEAR))\n",
    "    * XLNET utilizes Pernmutation Language Modeling\n",
    "        * Words are predicted on different permutations!\n",
    "* Since XLNET does away with explicit masking, we get super nice performance margins!\n",
    "    * However may not be the best as these permutations are costly to compute and process :(\n",
    "\n",
    "## Summary:\n",
    "* Transformers are complex encoder-decoder networks, based on self.attention.\n",
    "* BERT is a Transformer encoder\n",
    "* BERT derives attention-weighted word embeddings, using Masked Language Modeling, a complex attention mechanism and positional encoding.\n",
    "* BERT differs from Word2Vec by creating dynamic embeddings that discriminate between different contexts, and is similar in its downstream fine-tuning facilities.\n",
    "* XLNet differs from BERT by omitting the masking of words using a permutation language model, and it may be a better option in some circumstances, while being more costly from a computational point of view. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fafb746076eaa6f4a9c35dd52bd375d0d02af7c1d2f963d3faba917059a1fee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
