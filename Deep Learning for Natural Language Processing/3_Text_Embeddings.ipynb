{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "statutory-dressing",
   "metadata": {},
   "source": [
    "# Designing Data-Intensive Applications Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-acoustic",
   "metadata": {},
   "source": [
    "## Chapter 3: Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528a6dc",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "* Embeddings are procedures for converting input data into vector representations.\n",
    "    * A vector being like an arrow containing numbers\n",
    "\n",
    "#### Embeddings by Direct Computation: Representational \n",
    "* One-hot Embedding:\n",
    "    * Given a lexicon of N entries, every word is represented as a vector of N-1 zeroes with a single and distinct one\n",
    "    * Ideally, words that are related together should lie close to each other\n",
    "    * Note that this doesn't make snse from a linguistic pov as they all just differ by one bit\n",
    "\n",
    "#### Learning to embed: procedural embeddings\n",
    "* Keras Embeddings:\n",
    "    * Trainable layers that deplay a matrix of weights that are optimized during training\n",
    "    * They implicitly minimize loss functions via optimizing the representations they create\n",
    "        * Base criterion: Maximimze the distinctiveness of the vector representations\n",
    "    * Text -> Words via Tokenization -> Words as Integers via CountVectorizer -> Words as integers-> documents as vectors -> Embedding -> Word embeddings\n",
    "    * This can be easily visualized using T-SNE which maps high-dimensional vectors to lower planes\n",
    "    * Standard embeddings are in general randomly distributed\n",
    "        * But you can train the encodings to be finetuned to encode semantic/lexical relationships!\n",
    "        * Find the optimal embeddings for the words such that the provided document labeling is learned with maximum accuracy\n",
    "\n",
    "### From words to vectors: Word2Vec\n",
    "* Word2Vec takes word context information into account for establishing relations between words\n",
    "* Keras worked to maximize the accuracy in accordance to sentiment labeling but not for establishing relations between similar contexts.\n",
    "* Main functions:\n",
    "    * Predicting words from contexts\n",
    "    * Predicting contexts from words\n",
    "* In order to setup context prediction, we need both + and - contexts\n",
    "    * Negative sampling: Gather + examples and randomly generated negative examples\n",
    "* In order to get truly meaningful and convincing results, the model should be trained on a large amount of data for many iterations\n",
    "* You can also add embeddings to other embeddings!\n",
    "    * Stanford Uni:\n",
    "        * Word2Vec off the shelf\n",
    "        * 6 billion words w/ 400K vocab\n",
    "    * GloVe:\n",
    "        * 2014 version of English Wikipedia\n",
    "* Task-specific embeddings can be superior to general pre-trained embeddings under specific circumstances\n",
    "\n",
    "### From documents to vectors: Doc2Vec\n",
    "* Embeddings are not just restricted to the word level!\n",
    "* You can create a sliding window over words in a doc to generate n-grams of a pre-specified size\n",
    "    * The bector becomes co-trained with the words in the document/paragraph!\n",
    "* Functionally works in a very similar fashion to Word2Vec\n",
    "\n",
    "### Summary:\n",
    "* Embeddings can be optimized during optimization for an object, like training a sentiment classifier.\n",
    "* Pre-trained embeddings are not always beneficiary\n",
    "* Sometimes it makes sense to use an on-the-fly embeddings that is specific to and optimized for the NLP task at hand\n",
    "* Two examples of embedding algorithms are Word2Vec and Doc2Vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "fafb746076eaa6f4a9c35dd52bd375d0d02af7c1d2f963d3faba917059a1fee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
