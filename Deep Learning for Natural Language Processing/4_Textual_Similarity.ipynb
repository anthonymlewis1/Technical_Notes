{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Textual Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Common Applications\n",
    "    * Document Retrieval\n",
    "    * Topic Labeling\n",
    "    * Authorship Analysis\n",
    "\n",
    "# 4.1: The Problem\n",
    "* Scenario: Authorship analysis - who wrote this document?\n",
    "    * Scenario: Authorship attribution\n",
    "    * Scenario: Authorship verification\n",
    "\n",
    "# 4.2: The Data\n",
    "* Data:\n",
    "    * The PAN network public data\n",
    "    * Authorship attribution:\n",
    "        * Text snippets representing one author\n",
    "    * Authorship verification:\n",
    "        * Create pairs of documents belonging to the same or two authors\n",
    "        * How can we predict whether two documents belong to the same author?\n",
    "\n",
    "# 4.3: Data Representation\n",
    "* How can we derive a textual profile such that we can compare new texts with unknown authorship to known texts?\n",
    "    * We must determine the author style\n",
    "        * Word choice\n",
    "        * Word order and other grammatical choices\n",
    "        * Typos and abbreviations\n",
    "        * The use of adjectives/adverbs for sentiment\n",
    "* Representing this information\n",
    "    * Document -> Segmented Document -> Feature extraction -> Vectorization -> Vectors / Segment\n",
    "\n",
    "## 4.3.1: Segmenting documents\n",
    "* For a lot of words, split the document into fixed sized blocks of a number of words, then represent as a grouped vector.\n",
    "\n",
    "## 4.3.2: Word-level information\n",
    "* Hashing Trick -> Creates vectors of a specified length\n",
    "* Option 1: \n",
    "    * Vectorize the documents using bag-of-words\n",
    "    * Segment the documents using word n-grams\n",
    "* Option 2: \n",
    "    * Vectorize the documents using character n-grams\n",
    "    * Segment the documents based on character n-grams\n",
    "* If we preprocess our data explicitly, a CNN should be able to detect the higher-order n-grams.\n",
    "\n",
    "## 4.3.3: Subword-level information\n",
    "\n",
    "# 4.4: Models for Measuring Similarity\n",
    "\n",
    "## 4.4.1: Authorship Attribution\n",
    "* Goal: Let's train a deed multilayer perceptron + CNN on the authorship attribution task\n",
    "\n",
    "## Multilayer Perceptron\n",
    "* Train a classifier MLP on the authors in the PAN dataset for task A.\n",
    "    * Embedding connects to a Dense layer\n",
    "    * Dense layer feeds into a Dropout layer\n",
    "        * Dropout layer randomly deactivates neurons to omit over-fitting\n",
    "* We get OK accuracy for this, but adding more and more character limits to the n-grams help us lose valuable lexical information.\n",
    "* Altering the model to the explicit word n-grams approach leads to our lexicon exploding exponentially!\n",
    "\n",
    "## CNNs for Text\n",
    "* Authorship is expressed by many features scattered across a document. \n",
    "    * CNNs are good at picking up these features!\n",
    "* Train a CNN on the authors in the PAN dataset for task A.\n",
    "    * Embedding connects to a Dense layer\n",
    "    * Dense layer feeds into a Convolution1D layer\n",
    "* Certain choices for feature representation have a direct bearing on model complexity + resource demands!\n",
    "\n",
    "## 4.4.2: Authorship Verification\n",
    "* Siamese networks are networks with (usually 2) sister subnetworks + an arbiter\n",
    "    * One single model combineds the two input layers such that they share embeddings and processing!\n",
    "    * The two sister networks share the same weights!\n",
    "    * The latent representations are used by the arbiter\n",
    "        * The arbiter computes a contrastive loss between the representation\n",
    "        * The overall network learns a threshold to decide whether the measured distance indicates similarity\n",
    "            * Distance is measured based on an exponentiated negative sum of differences\n",
    "    * The network trains on pairs of similar or dissimlar texts.\n",
    "\n",
    "## 4.5: Summary\n",
    "* Textual similarity use cases include authorship attribution and authorship verification\n",
    "* Representational choices can have hefty ramifications for model complexity. Some models may bypass some of that burden by their intrinsic organization (like MLPs compared to CNNs).\n",
    "* Siamese networks can be used to determine textual similarity.\n",
    "* Lexican information only is not sufficient for establishing textual similarity. Style is expressed both lexically and formally, in terms of word combinations.\n",
    "* CNNs are good in emphasizing sequential information. They outperformed a simple MLP in our authorship attribution experiments.\n",
    "* Siamese networks can be used for textual similarity in authorship verification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fafb746076eaa6f4a9c35dd52bd375d0d02af7c1d2f963d3faba917059a1fee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
