{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Transforomers: hands-on with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Introduction: Working with BERT in practice\n",
    "* The costs to pretrain BERT and other transformer-models from scratch on large amounts of data can be expensive :(\n",
    "    * The original authors would have spent $10K for BERT\n",
    "    * XLNET: $60K\n",
    "    * GPT-3 Transformer: $4.6M\n",
    "* Luckily, smaller pretrained models are becoming available for free!\n",
    "    * Ideally, that means that you would download a pre-trained model, incorporate\n",
    "\n",
    "## 10.2 A BERT layer\n",
    "* In deep learning networks, BERT layers can be used on top of an input layer!\n",
    "    * They encode words in the input layer\n",
    "* To work with BERT, we need\n",
    "    * A pre-trained BERT model\n",
    "    * A facility for importing such a model and expositing it to the rest of our code\n",
    "* Thank Google for Tensorflowhub!\n",
    "    * A site for downloading models and other pre-constructed deep learning networks.\n",
    "* When working with BERT, we need to balance time complexity and quality\n",
    "\n",
    "## 10.3 Training BERT on your own data\n",
    "* There are many Python frameworks based on Keras that allow us to work w/ BERT\n",
    "    * fast-bert\n",
    "    * keras-bert\n",
    "* Start with a list of normal sentences\n",
    "    * In this case we're going to use Edgar Allen Poe's The Cask of Amontillado.\n",
    "    * We can choose to split on '.' but we can also delimit on other closing punctuation to get additional (pseudo-)sentences.\n",
    "* Next create the BERT data\n",
    "    * We generate this utilizing a generator so that we're not limited by the machine's storage capacities.\n",
    "    * Under the hood, keras_bert will insert the CLS and SEP delimiters within our paired sentence data.\n",
    "* Overall process:\n",
    "    * Tensorflow Hub: Load Tokenizer\n",
    "        * Tokenize data, mask data, generate segment positions, gather label of input example\n",
    "        * Processed Data\n",
    "    * Data -> Process Data\n",
    "        * convert_examples_to_features\n",
    "* For our complimentary training tasks:\n",
    "    * Some stored sentiment data\n",
    "        * I hate pizza, negative\n",
    "\n",
    "## 10.4 Fine-tuning BERT\n",
    "* Pick up a pre-trained mode and use a labeled dataset!\n",
    "    * BERT added as a layer that learns the labeling task\n",
    "\n",
    "## 10.5 Inspecting BERT\n",
    "* Unlike Word2Vec, BERT can identifiy homonyms since it uses more contextual vectors!\n",
    "\n",
    "## 10.6 Applying BERT\n",
    "* Large models like BERT/Word2Vec can showcase bias due to the sources underlying the models.\n",
    "    * Counteractable utilizing\n",
    "        * Word-Embedding Association Test\n",
    "        * Relational Inner Product Association Test\n",
    "\n",
    "# 10.7 Summary\n",
    "* Existing BERT models can be imported into your Keras network.\n",
    "* You can train BERT on your own (raw text) data.\n",
    "* Fine-tuning BERT models on additional labeled data (downstream tasks) may be beneficiary.\n",
    "* As with any data-driven model, BERT is susceptible to bias, and may produce undesirable associations between words, reflecting cultural and societal biases that have crept into the raw data underlying a BERT model. It is important to be aware of this as an NLP engineer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fafb746076eaa6f4a9c35dd52bd375d0d02af7c1d2f963d3faba917059a1fee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
