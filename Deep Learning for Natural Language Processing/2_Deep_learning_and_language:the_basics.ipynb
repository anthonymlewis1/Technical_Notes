{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceramic-acoustic",
   "metadata": {},
   "source": [
    "## Chapter 2: Deep learning and language: the basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528a6dc",
   "metadata": {},
   "source": [
    "## Basic Architectures of Deep Learning\n",
    "\n",
    "### Deep Multilayer Perceptrons (MLP)\n",
    "* Consists of MLP layers containing artifical neurons\n",
    "    * Artificial Neurons: Mathematical functions that receive input from weighted connections to other neurons.\n",
    "        * ANs produce output values through a variety of mathematical operations.\n",
    "    * Typically a depp network will have many neurons with lots of weights to calculate!\n",
    "    * While there is no magic number, networks with more than 2 layers may be deemed 'deep'.\n",
    "* Sequential Model: A container for a stacked set of layers and facilities for defining layers.\n",
    "* This is the plain vanilla version of neural networks!\n",
    "* Creating a Model:\n",
    "    * Fully Connected: When dense layers connect all incoming neurons from one layer to the next layer.\n",
    "    * Tensor: Container for numerical data\n",
    "    * Important Features:\n",
    "        * Batch Size: Determines the grouping of data points in batches that are handled collectively during training.\n",
    "        * Loss Function: Function that computes mismatches between predictions and reality alongside the labels it should assign according to the training data.\n",
    "        * Numerical Optimizer Algorithm: Carries out the gradient descent process\n",
    "        * Evaluation Metric: Performs intermediate evaluation of the model\n",
    "        * Loss: The loss of the loss function\n",
    "            * Ex: Accuracy, MSE, etc.\n",
    "        * Cost Function: Function that takes all of the current weights and determines the 'lousiness' of the weights based on the training examples.\n",
    "        * Gradient Descent: Function that takes the cost function and minizmies it.\n",
    "\n",
    "### Two Basic Operators: Spatial and Temporal\n",
    "* Spatial Filtering:\n",
    "    * Addresses properties of the structure of input data\n",
    "    * Weeds out irrelevant patches and lets valuables ones through\n",
    "* Convolutional Neural Network (CNN):\n",
    "    * Applies a set of convolutions to input data and learns the weights of these filters on the basis of training data.\n",
    "        * Convolutions: Weighted filters\n",
    "    * The convolutions scan the data on multiple points and gradually extract out the important pieces of information.\n",
    "        * Imagine that this scanning is like looking at a small subsection of the image that shifts over each stride:\n",
    "            * Pixel step size\n",
    "    * The return of these filters is just a weight matrices!\n",
    "        * The filters merely emphasize or deemphasize input\n",
    "    * While you start with a ton of initial random weights, the network will eventually figure out which weights are better and become optimized.\n",
    "    * Activation Operations:\n",
    "        * reLu: Max(x, 0)\n",
    "        * Sigmoidal\n",
    "        * Max Pooling: Return the maximum value \n",
    "    * Dimensionality Reduction of Feature Representations!\n",
    "        * Converts representations of a certain dimension into lower dimensional representations.\n",
    "    * Hyperparameters:\n",
    "        * Dimensionality of the Output Space\n",
    "        * Kernel Size: Size of the filter\n",
    "        * Stride: Stepsize\n",
    "        * These valeus are typically chosen arbitrarily or estimated based on validation data\n",
    "    * Layers:\n",
    "        * Dense: Binary representation of the output labels\n",
    "        * Dropout: Randomly resets a specified faction of the input units to 0 to prevent overfitting\n",
    "    * Results:\n",
    "        * Epoch: Full sweep through the training data\n",
    "        * Binary Cross-Entropy: Expresses how well a classification model producing probabilities performs.\n",
    "        * Time: Amount of time spent on the epoch\n",
    "        * Acc: Accuracy of the model during training\n",
    "* Temporal Filtering:\n",
    "    * Like Spatial Filtering but with information from the past\n",
    "        * The depth here extends across the horizontal direction of a timeline.\n",
    "* Recurrent Neural Networks (RNN):\n",
    "    * Similairities to CNN:\n",
    "        * Selection process boils down to learning weight matrices\n",
    "        * Emphasis/de-emphasis of certain aspects of the info\n",
    "    * RNNs memorize the decisions made in the past!\n",
    "        * Allows for the ability to implement bias and carry out classifications in line with what was done in the past\n",
    "    * Simple RNNs:\n",
    "        * Neural network with a limited amount of memory\n",
    "        * At any time, an RNN memory state is determined by:\n",
    "            * Memory state at the previous time tick\n",
    "            * A weight matrix weighting the previous memory state\n",
    "            * A weight matrix weighting the current input to the RNN\n",
    "        * The depth of the network is related to the amount of inputs!\n",
    "        * Weights here are shared/updated across all inputs!\n",
    "            * This allows the network to 'learn' from previous experiences. \n",
    "            * The weights are now minimized globally to reduce training error\n",
    "        * Example: Predict the next character for a string, based on characters that precede that one\n",
    "            * Onehot-encode the characters which get fed into the RNN\n",
    "            * These units form a temporal chain which feeds into the next hidden unit\n",
    "            * The one-hot vecotrs are then reconstructed from the hidden unit outputs\n",
    "        * RNNs are quite crude!\n",
    "            * They fail on long sequences\n",
    "            * They blindly reuse hidden states which means that it can't tell apart trash/valuable data.\n",
    "    * Long Short Term Memories (LSTM):\n",
    "        * This was developed to help fix the problems associated with RNNs\n",
    "            * Fixed via adding gating operations\n",
    "            * These gates decide which historical information should be used for local processing in the current input\n",
    "        * Every LSTM consists of a number of cells, chain in sequence, which consumes the same input\n",
    "            * The number of cells is a hyperparameter which is up to the engineer to determine\n",
    "        * Since it's encoding the context of the character/word, this info again is available globally.\n",
    "        * 9 Important Weight Matrices\n",
    "            * Weight on the input gate, applied to the input\n",
    "            * Weight on the output gate, applied to the input\n",
    "            * Weight on the forget gate, applied to the input\n",
    "            * Weights on the input, computing the activity of the entire cell\n",
    "            * Input gate weights applied to the previous hidden state\n",
    "            * Weights applied to the hidden state of the net, for computing the activity of the cell\n",
    "            * Forget gate: Applied to the previous hidden state of the net\n",
    "            * Weight on the output gate, applied to the previous hidden state of the net\n",
    "            * Weights applied to the cell activity\n",
    "        * Inputs to an LSTEM:\n",
    "            * Samples: Amount of data blocks\n",
    "            * Timesteps: Number of observations / batch\n",
    "            * Features: Dimensionality in each observation\n",
    "        * When doing this on characters that repeat multiple times (almost randomly), the network figures out when to produce suffixes and is able to condition the suffixes on old information! \n",
    "\n",
    "### Deep Learning and NLP: A New Paradigm\n",
    "* Deep Learning is not just a revamped, highly performant type of machine learning!\n",
    "    * Spatial filtering: Allows for the emphasis/deemphasis of parts of data\n",
    "    * Temporal filtering: Allows historical information to be gated in a keep/forget manner\n",
    "    * You can also combine the RNNs with CNNs to get many different combinations!\n",
    "\n",
    "### Summary:\n",
    "* Basic deep learning architectures are multilayer perceptrons, spatial (convolutional) and temporal (RNN and LSTM-based) filters.\n",
    "* Convolutional and Recurrent Neural Networks, as well as Long Short Term-based networks, can be easily coded in Keras.\n",
    "* Deep learning examples applied to language analysis in this chapter include sentiment analysis and character prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "fafb746076eaa6f4a9c35dd52bd375d0d02af7c1d2f963d3faba917059a1fee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
