{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a528a6dc",
   "metadata": {},
   "source": [
    "## Chapter 5: Sequential NLP and memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Goal: How can we answer a question on the basis of a number of facts?\n",
    "\n",
    "## 5.1 Memory and language\n",
    "* Language is a sequential and contextual phenomenom!\n",
    "    * Part-of-speech tagging\n",
    "    * Sentiment and topic analysis\n",
    "### 5.1.1 The problem: Question Answering\n",
    "* Question Answering involves matching answers to questions\n",
    "* This task is dependent on memory as it must store all of the facts since it doesn't know the upcoming question.\n",
    "* Goal: Given a sequence of sentences, and a question that can be answered by 1 (and only 1) of the sentences, how can we retrieve the necessary information for answering the question?\n",
    "\n",
    "### 5.2: Data and Data Processing\n",
    "* bAbI dataset\n",
    "    * Sequences of facts, linked to questions\n",
    "    * Created by Meta\n",
    "* Example Story:\n",
    "    * Mary moved to the bathroom\n",
    "    * John went to the hallway\n",
    "    * Where is Mary? bathroom\n",
    "* Storing information like this can be burdensome so check the model under two conditions:\n",
    "    * Using a question and the supporting fact\n",
    "    * Using all facts in a story including non-relevant ones\n",
    "* Data Preparation:\n",
    "    * Prepare the following vectors:\n",
    "        * List for all facts\n",
    "            * Lumped into one long vector\n",
    "        * List for all vectorized questions\n",
    "        * List of labels\n",
    "    * The vectors will be created via a tokenizer\n",
    "        * Tokenizer fitted on the words in the training/test data\n",
    "    * Create a switch to remove/keep irrelevant facts\n",
    "\n",
    "## 5.3: Question Answering w/ Sequential Models\n",
    "### 5.3.1: RNNs for Question Answering\n",
    "* RNN -> Recurrent Neural Network\n",
    "    * Blindly passes all historical information in\n",
    "* Implement a branching model w/ 2 RNNs\n",
    "    * 1 RNN handles the facts/stories\n",
    "    * 1 RNN handles the question\n",
    "* It's important that we use One-hot encoding on here such that our question/answer vectors can be related\n",
    "* Flow:\n",
    "    * Create 2 RNN input layers\n",
    "    * Merge the input layers via concatenation\n",
    "    * Send it through a Dense layer\n",
    "    * Get an output layer w/ dimensionality vocab_size\n",
    "    * Compile the model + test\n",
    "* Model accuracy is super high if we throw out the irrelevant facts\n",
    "* But when we start needing more context, we get a significant drop in performance as it can't store as much information with a binary switch\n",
    "* Incremental Context:\n",
    "    * Rather than using a binary switch, specify the amount of irrelevant facts we can tolerate\n",
    "    * Slowly increate the numbers of irrelevant facts + amount of words the facts cover\n",
    "    * Even still, RNNs suck for holding lots of information so the performance isn't that great\n",
    "### 5.3.2: LSTMs for Question Answering\n",
    "* LSTM -> Long Short Term Memory\n",
    "    * Works 1 feature at a time through an input vector\n",
    "    * Updates its cell state at each step\n",
    "* LSTMs come in either stateless or stateful mode\n",
    "    * Stateless:\n",
    "        * After a sequence has been processed, the weights of the surrounding layers are updated through backpropagation\n",
    "        * Cell state of the LSTM is reset\n",
    "    * Stateful:\n",
    "        * Vectors across batches are synchronized\n",
    "        * Each bector proceeds with the cell state for a corresponding vector\n",
    "        * Batches contain temporally linked vectors\n",
    "    * In both modes, a batch of labeled training vectors are used\n",
    "* LSTMs require that we have the following data triplets:\n",
    "    * Number of samples\n",
    "    * Time Steps\n",
    "    * Features / Timestep\n",
    "* Stateful batches are NOT useful here as we're not trying to predict an outcome per fact.\n",
    "    * Question: Would it eb useful then if we needed information from multiple facts to get the answer? Thinking 'Where is A and B?'\n",
    "* Flow:\n",
    "    * Almost exactly the same as RNNs but with LSTM layers instead\n",
    "    * Add an input layer prior to the LSTM to deduce the triplet data structure\n",
    "* Again, model accuracy is high if we throw out irrelevant information, but it still has poor performance when running on all contexts\n",
    "* LSTMs outperform RNNs for moderately long contexts, but fail on extensive sequences\n",
    "### 5.3.3 End-to-end memory networks for Question Answering\n",
    "* Moving from rote mapping to responsive memory\n",
    "* Don't just teach a network to predict an answer from the combined story/question vector\n",
    "    * Instead, produce a memory response and use that to weight the facts vector\n",
    "    * The question vector is then recombined with the weighted factors vector and used to predict a word index\n",
    "* Approach:\n",
    "    * Facts are embedded with an embedding A\n",
    "    * Question is embedded with an embedding B\n",
    "    * Facts are also embedded with an embedding C\n",
    "    * Create the memory bank response by deriving an inner product of the facts with A embedding\n",
    "    * Combine the probabilities of A w/ C using a weighted sum operation\n",
    "    * The question in B is now combined through weighted sum concatenation to the above\n",
    "        * Now the facts are weighted for relevance at answering the question!\n",
    "    * The resulting concatenation is sent to a dense output layer to get the word index of the answer\n",
    "* The resulting model gives a greater performance for super long sequences\n",
    "\n",
    "\n",
    "## 5.4: Summary\n",
    "* Three approaches to Question Answering with differing memory capacity involve using RNNS, LSTMs, and end-to-end memory networks\n",
    "* For Question-Answering, RNNs perform worse than LSTMs in remembering long sequences of data.\n",
    "* End-to-end memory networks work by memorizing sample questions, supporting facts and answers to the questions in memory banks.\n",
    "* End-to-end memory networks outperform LSTMs for Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fafb746076eaa6f4a9c35dd52bd375d0d02af7c1d2f963d3faba917059a1fee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
