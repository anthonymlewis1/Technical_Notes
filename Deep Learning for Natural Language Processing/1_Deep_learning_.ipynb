{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceramic-acoustic",
   "metadata": {},
   "source": [
    "## Chapter 1: Deep learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e345cb",
   "metadata": {},
   "source": [
    "## 1.1 A Selection of Machine Learning Methods for NLP\n",
    "\n",
    "* Classification Goal: To arrive at linear separability of data that is labeled with classes\n",
    "    * Classes: Labels that indicate (a usually exclusive) category to which points belong\n",
    "    * Input Space: Vector representations of descriptive traits\n",
    "    * Feature Space: Processing, manipulation, and abstraction of the input space during the learning stage\n",
    "    * Outer Space: Class labels that separate the various data points based on class boundaries\n",
    "* Input Space -> Feature Space -> Output Space\n",
    "    * Through deep learning, the relationships between the input and the output are defined\n",
    "* Training a machine larning component involves learning boundaries between classes.\n",
    "    * Linear Classifier: A linear function that separates classes with a straight line\n",
    "\n",
    "### The Perceptron (Neural / Cognitive)\n",
    "* Thought: What if you had a vector of features that descrives aspects of a certain object and you wanted to create a function to turn these features into a binary label\n",
    "    * Eg: Words in a document -> +/- sentiment\n",
    "* Rosenblatt's perceptron\n",
    "    * Biologically inspired machien learning component\n",
    "    * Apparatus: 20x20 photosensitive cells\n",
    "    * Weights: Set by electromotors driving potentiometers\n",
    "    * Learning: One-layer neural network\n",
    "* Example Perceptron: \n",
    "    * Build a document classifier that categorizes raw texts as atheist or medical\n",
    "    * Logic:\n",
    "        * Make a subselection for two newsgroups of interest\n",
    "        * Train a simple perceptron on a vector representation of the documents\n",
    "    * Code:\n",
    "        * Import sklearn's basic perceptron classifier\n",
    "        * Import and filter down the perceptron\n",
    "        * Fit the CountVectorizer onto the training data\n",
    "        * Compute TF.IDF representations of the count vectors\n",
    "            * TF.IDF -> Documents into vectors for ML modeling\n",
    "        * Train the perceptron on the TF.IDF vectors\n",
    "        * Convert the test data into a form for the perceptron\n",
    "        * Apply the perceptron to the results\n",
    "        * Print the results\n",
    "* IN REAL LIFE, TOPICS ARE NOT EASY ENOUGH TO SEPERATE LINEARLY\n",
    "\n",
    "### Support Vector Machines (Eager)\n",
    "* Thought: What if you could add an additional dimension to object perception such that you create linear boundaries between data points in that dimension? \n",
    "* An SVM is a binary classifier that maps data using a kernel function in feature space to higher dimensions in which data is separable by a hyperplane.\n",
    "    * Kernel functions transform the input space into an alternative representation that has a higher dimensionality with the aim of making data linearly separable.\n",
    "* Kernel Function:\n",
    "    * A kernel function computes a product between two vectors.\n",
    "        * This product is a number expressing a relation between two input vectors.\n",
    "    * Takes two vectors, mixes in a constant, and produces a specific form of a dot product of the two vectors.\n",
    "    * Example quadratic kernel: K(x,y) = (c+x^T * y)^2\n",
    "* Kernel Trick:\n",
    "    * Hopefully in the higher-dimensional space, things become easier to separate. This is allowed to be used as you're not explicitly transforming the data.\n",
    "* Two classes are at best separated with maximally wide boundaries (maximal margins)\n",
    "* Support Vectors: The data points determining the slope of these boundaries\n",
    "    * Learning weights that optimize the margins with the least error is what SVMs do!\n",
    "* SVMs are eager because they throw away a lot of their training data and only care about the support vectors. Eager is compact and representational of the training data.\n",
    "\n",
    "### Memory-Based learning (Lazy)\n",
    "* Lazy in that it does not generalize training data and keeps all training data in memory\n",
    "* Similar to SVM in that they also still compute distance measures for simularity.\n",
    "    * But no dimensionality tricks!\n",
    "* IB1 Distance Metric:\n",
    "    * Computes the distance between feature vectors based on exact similarity for non-numeric values. Matches get 0 else 1\n",
    "    * Find the training data with the same distance to the current test item\n",
    "    * Then you would vote to see which is the most probably label for the test item.\n",
    "        * K parameter lets you limit how many similar items you look at.\n",
    "        * K-nearest distances NOT k-nearest neighbors\n",
    "* Keeping everything in memory allows for exception handling to occur!\n",
    "    * Eager ML models tend to compile away exceptions\n",
    "\n",
    "## 1.2 Deep Learning\n",
    "\n",
    "* Deep Learning: A neural network with lots of internal/hidden layers and specific filtering operations\n",
    "    * Very effective statistical technique for working with (very) many parameters.\n",
    "        * Millions!\n",
    "    * General Architecture\n",
    "        * Input Data -> Layer 1 -> Layer 2 ... -> Layer N -> (Output label 1 .. Output label n)\n",
    "    * Hierarchical representations of data\n",
    "        * \"Lower\" layers get fed into \"higher\" layers\n",
    "            * Layers = complex functions processing inputs and weights\n",
    "            * Weights encode the importance of the information within the network\n",
    "                * These weights are estimated and fine-tuned by neurons\n",
    "                    * Neurons = Basic processing units of a neural network\n",
    "        * Once the layers are complete, the network produces probabilities for the possible outcomes\n",
    "            * All layers are hidden b/c they can't be readily observed except the input and output.\n",
    "            * The possibility with the highest probability gets the final output label.\n",
    "    * Developments in Deep Learning: \n",
    "        * Restricted Boltzmann Machines (RBMs):\n",
    "            * Issue: Vanishing Gradient\n",
    "                * When there are so many parameters that weight adjustments become too tiny to be useful.\n",
    "                * Repeated multiplication of small numbers going from layer to layer just eventaully 'dissapear' b/c they're not useful\n",
    "            * RMB: Complete networks that learn probability distributions from data\n",
    "                * You setup the RMB s.t. every layer sends its hidden layer data as input to the next layer rather than the hidden layer sending itself up as input.\n",
    "            * Now that there's layer-wise training and the graidents don't travel as far, the vanishing gradient is removed.\n",
    "        * Rectified Linear Unit (ReLU):\n",
    "            * Issue: Having a function that allows for your function to learn both is computationally expensive and may be super complicated.\n",
    "            * Very simple function that returns the max of the value or 0\n",
    "                * This eliminates all negative numbers\n",
    "            * Allows for increased speed and scalability of the network computations\n",
    "*  Advantages:\n",
    "    * Repeated application of data decluterring\n",
    "    * Great for handling sequential information with memory operators and buffers\n",
    "\n",
    "## 1.3 Vector representations of language\n",
    "* Since ML is all about measuring distances between objects in multi-dimensional spaces, we must convert all text to vectors. They must be computed directly and exaclty from data.\n",
    "* Vector representations:\n",
    "    * Representational vectors:\n",
    "        * Represents text by describing them across a number of human-interpretable feature dimensions.\n",
    "        * Ex:\n",
    "            * hospitaal hospitaal+tje and woning wonin+kje\n",
    "            * +,h,O,s,-,p,i,=,-,t,a,l,T\n",
    "            * In this representation, words receive dummy values for absent dimensions\n",
    "            * Words over the 12 characters are truncated\n",
    "        * CountVectorizer:\n",
    "            * Mas words to vector positions st every word gets a unique position\n",
    "        * OneHot Vector Encoding:\n",
    "            * On a sparsely populated N-dimensional vector\n",
    "            * Every word is represented by a digit\n",
    "    * Operational vectors\n",
    "        * Derived representation of data as produced by an algorithm\n",
    "        * Term Frequency Inverse Document Frequency (TF IDF):\n",
    "            * Words are weighted with numerical scores based on saliency\n",
    "            * Frequency of the word in a document x Frequency of the word in other documents.\n",
    "                * Higher scores indicate\n",
    "                * The lower the score, the lower the specialness of the word as it appears a lot.\n",
    "        * Neural Word Embeddings\n",
    "            * Embeddings are produced by neural networks that predict context-based words\n",
    "            * Popular input to deep neural networks.\n",
    "            * Distributional Semantic Similarity:\n",
    "                * Associate words together based on their context then associate the vectors with relatively similar scores\n",
    "            * Shallow: 1 Hidden Layer + 1 Input Layer\n",
    "            * Deep: >1 Hidden Layer or >1 Input Layer\n",
    "            * Thus each embedding represents a phrase that geometrically corresponds to a centroid of a vector space spanned by all the word embedded bectors of its words. \n",
    "            * Sample Flow:\n",
    "                * Text -> Words via Tokenization\n",
    "                * Words -> Vectors via Word2Vec\n",
    "                * Vectors -> Documents as Vectors via Averaging\n",
    "                * Documents as Vectors -> Maching Learning algorithm\n",
    "\n",
    "## 1.4 Vector Sanitization\n",
    "### The Hashing Trick\n",
    "* Large vectors are unwieldy to handle!\n",
    "    * Large memory allotments w/ sparsely populated dimensions\n",
    "* Feature Hashing:\n",
    "    * Map every feature to an index and the algorithm updates the info at those indices only\n",
    "    * Utilize an inverted lexicon (Integers -> Words)\n",
    "        * Similar input values will lead to similar numerical indices\n",
    "            * Amount of similarity dependent on the specific hash function\n",
    "\n",
    "### Vector Normalization:\n",
    "* Vectors represent quanitities as a magnitude and a direction\n",
    "* Unit Vector: Vectors that have been squeezed into a subspace which reduces the variance across dimensions.\n",
    "* Normalization is good for forcing vectors to be within the same data range which reduces sensitivity to outlier data!\n",
    "\n",
    "## Summary\n",
    "* Many different forms of NLP are rooted in machine learning and statistics.\n",
    "* Deep learning traces back to teh 1960s, but it only became operational a few decades later.\n",
    "* Text needs to be vectorized in order for machine learning to perform natural language processing\n",
    "* While many options are open for vectorization, inferring and optimizing vectorization from data within machine learning is preferable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528a6dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "fafb746076eaa6f4a9c35dd52bd375d0d02af7c1d2f963d3faba917059a1fee8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
