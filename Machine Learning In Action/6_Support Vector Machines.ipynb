{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Support Vector Machines\n",
    "* Considered to be the best stock/vanilla classifier\n",
    "    * Stock = not modified\n",
    "\n",
    "## Separating data with the maximum margin\n",
    "* Pros:\n",
    "    * Low generalization error\n",
    "    * Computationally inexpensive\n",
    "    * Easy to interpret results\n",
    "* Cons:\n",
    "    * Sensitive to tuning parameters / kernel choice\n",
    "    * Natively only handles binary classification\n",
    "\n",
    "* Linearly separable:\n",
    "    * When data can be separated enough via a straight line\n",
    "    * Not:\n",
    "        * Data subsection aka one piece of data in another\n",
    "        * Overlapping data\n",
    "    * Separating hyperplane: Line used to separate the dataset\n",
    "        * Hyperplane: A decision boundary where data on one side belongs to one class and data on the other side belongs to the other class\n",
    "\n",
    "* Assumption: The farther way a data point is from the decision boundary, the more confident we are about the prediction\n",
    "* We will want to maximize the amount of margin so that our model is robust even with small sample sizes\n",
    "    * Margin: Distance between point closest to the separating hyperplane to the separating line\n",
    "* Goal: Maximize the distance from the separating line to the support vectors\n",
    "    * Support Vectors: Points closest to the separating hyperplane\n",
    "\n",
    "## Finding the maximum margin\n",
    "* How can we measure the line that best separates the data?\n",
    "    * Hyperplane form: w^T * x + b\n",
    "* Switching class labels of 0/1 to -1/1 will allow us to use a single equation for this\n",
    "* Margin calculation: label * (w^T * x + b)\n",
    "    * Far away points will have large absolute values\n",
    "* Goal: Find the w/b values w/ the smallest margin than maximize that margin\n",
    "\n",
    "max(x,b) { min(label * (w^T * x + b)) * 1 / abs(||w||)}\n",
    "\n",
    "* Since optimizing multiplications is nasty, instead hold label * (w^T * x + b) to be 1 so that we can maximize ||w||\n",
    "* We can then solve these using Lagrange multipliers to regrame the hyperplane to be in terms of the data points!\n",
    "    * Makes one huge assumption: The data is 100% linearly separable\n",
    "    * Slack variables: Examples on the wrong side of the decision boundary\n",
    "\n",
    "## Efficient optimization with the SMO algorithm\n",
    "\n",
    "### Platt's SMO algorithm\n",
    "* SMO: Sequential Minimal Optimization\n",
    "* Breaks large optimization into smaller chumps than breaks it into many small problems.\n",
    "* Process:\n",
    "    * Choose two suitable alphas to optimize each cycle\n",
    "    * Increase one alpha and decrease the other\n",
    "    * Criterion:\n",
    "        * Alphas need to be outside their margin boundary\n",
    "        * The alphas are not clamped/bounded\n",
    "\n",
    "```\n",
    "# Pseudocode\n",
    "Create an alphas vector filled w/ 0s\n",
    "While the number of iterations is less than MaxIterations:\n",
    "    For every data vector in the dataset:\n",
    "        If the data bector can be optimized:\n",
    "            Select another data vector at random\n",
    "            Optimize the two vectors teogether\n",
    "            If the vectors can't be optimized -> break\n",
    "    If no vectors were optimized -> increment the iteration count\n",
    "```\n",
    "\n",
    "# Summary"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
