{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Splitting datasets one feature at a time: decision trees\n",
    "\n",
    "* The most commonly used technique!\n",
    "* Humans can very easily understand the data\n",
    "* Basis:\n",
    "    * Decision Tree: Flowchart of decisions to make\n",
    "    * Decision Blocks: Specific decision to make\n",
    "    * Terminating Block: Conclusion reached\n",
    "    * Branches: Links that lead to other decision blocks or a terminating block\n",
    "* The machine creates the rules from the dataset and also will classify your information!\n",
    "\n",
    "## Tree Construction\n",
    "* Pros:\n",
    "    * Computationally cheap\n",
    "    * Easy for humans to understand learned results\n",
    "    * Missing values OK\n",
    "    * Can deal with irrelevant features\n",
    "* Cons:\n",
    "    * Prone to overfitting\n",
    "* Works with:\n",
    "    * Numeric values\n",
    "    * Nominal values\n",
    "\n",
    "* To build a decision tree you need to determine which features should split the data\n",
    "* Pseudocode:\n",
    "    * Check if every item in the dataset is in the same class (yes/no)\n",
    "    * Yes:\n",
    "        * Return the class label\n",
    "    * No\n",
    "        * Find the best feature to split the data\n",
    "        * Split the dataset\n",
    "        * Create a branch node\n",
    "        * For each split:\n",
    "            * Call originalfunction and add the result to the branch node\n",
    "        * Return the branch node\n",
    "\n",
    "* General Approach:\n",
    "    * Collect: Any method\n",
    "    * Prepare: Works only on nominal values so continuous values must be quantized\n",
    "    * Analyze: Visually inspect the tree after it is built\n",
    "    * Train: Construct a tree data structure\n",
    "    * Test: Calculate the error rate with the learned tree\n",
    "    * Use: Understand the data\n",
    "\n",
    "* Information Gain: The change in information before and after a split\n",
    "* Entropy: The measure of information of a set\n",
    "    * information of x = -log2p(x):\n",
    "        * p(x) = probability of choosing this class\n",
    "    * Entropy of x = - Sum(p(x) * log2(p(x)) from 1 to n:\n",
    "        * n = number of classes\n",
    "    * The higher the entropy the more mixed up the data\n",
    "\n",
    "* Flow:\n",
    "    * Split the dataset based on the best attribute to split\n",
    "    * Stop when you run out of attributes to split or the branch instances are the same class\n",
    "        * Create a terminating block / leaf node\n",
    "* Overfitting: The probability that a model matches the training data too well\n",
    "\n",
    "## Summary:\n",
    "* Decision tree classifier is a work-flow diagram with the terminting blocks = classification decisions\n",
    "* Measure the entropy of a data until all the data is in the same class\n",
    "* Recusion is used to turn a dataset into a decision tree\n",
    "* Python pickely is used to persist the tree\n",
    "* Overfitting can be removed by pruning the decision tree\n",
    "    * Combining adjacent leaf nodes that don't provide a large information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
