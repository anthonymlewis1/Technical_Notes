{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Logistic Regression\n",
    "\n",
    "* Logistic Regression: Building an equation to do classification\n",
    "    * Aim: Try to find the best-fit set of parameters\n",
    "\n",
    "## Classification with logistic regression and the sigmoid function: a tractable step function\n",
    "* Pros:\n",
    "    * Computationally inexpensive\n",
    "    * Easy to implement\n",
    "    * Knowledge representation easy to interpret\n",
    "* Cons:\n",
    "    * Prone to underfitting\n",
    "    * May have low accuracy\n",
    "* Works with:\n",
    "    * Numeric values\n",
    "    * Nominal values\n",
    "\n",
    "### 2 Feature Graphs\n",
    "* Represented by a sigmoid equation:\n",
    "    * sigma = 1 / (1 + e^-z)\n",
    "* Classification:\n",
    "    * Normalize the weights then sum them\n",
    "    * Place the results into a sigmoid\n",
    "    * Results akin to a probability\n",
    "        * X > .5: Classified as 1\n",
    "        * X < .5: Classified as 0\n",
    "    \n",
    "## Using optimization to find the best regression coefficients\n",
    "* Z = w^T * x\n",
    "    * Equivalent to z = w0x0 + w1x1 + ... + wnxn\n",
    "    * x is input data\n",
    "* We want to find the best coefficients w to be as successful as possible.\n",
    "\n",
    "### Gradient Ascent\n",
    "* Idea: The best way to find the maximum of a function is to move in the direction of the gradient\n",
    "* delta(f(x,y)) = (f(x,y)/x , f(x,y)/y)\n",
    "    * Moving in the x direction by amount f(x,y)/x\n",
    "    * Moving in the y direction by amount f(x,y)/y\n",
    "* The gradient operator will always point in the direction of the greatest increase.\n",
    "* Gradient Ascent algorithm: w:= w + a DELw f(w)\n",
    "    * Step size: a\n",
    "* Repeated until a stopping condition is reached:\n",
    "    * Specified number of steps\n",
    "    * Certain tolerance margin\n",
    "* Gradient Descent algorithm: w:= w - a DELw f(w)\n",
    "    * For function minimization\n",
    "\n",
    "```\n",
    "# Assumptions\n",
    "Data in form (x1, x2, classifier)\n",
    "\n",
    "# Pseudocode\n",
    "Start with the weights all set to 1\n",
    "Repeat R times:\n",
    "    Calculate the gradient of the entire dataset\n",
    "    Update the weights vector by alpha*gradient\n",
    "    Returns the weights vector\n",
    "```\n",
    "* Goal: Solving for a set of weights to make a line that seperates the different classes of data\n",
    "\n",
    "### Stochastic Gradient Ascent\n",
    "* Gradient ascent is very inefficient process-wise\n",
    "    * Instead update the weights using only one instance / point at a time\n",
    "    * Online learning algorithm\n",
    "        * Online: Updating the classifier as new data comes in\n",
    "        * Batch: Processing data all at once\n",
    "\n",
    "```\n",
    "# Assumptions\n",
    "Data in form (x1, x2, classifier)\n",
    "\n",
    "# Pseudocode\n",
    "Start with the weights all set to 1\n",
    "Repeat R times:\n",
    "    Calculate the gradient of one data point\n",
    "    Update the weights vector by alpha*gradient\n",
    "    Returns the weights vector\n",
    "```\n",
    "\n",
    "* How can we optimize this?\n",
    "    * 1 pass is never enough as it may misclassify a large number of samples\n",
    "    * Let's instead run this many times to try and reach a convergence\n",
    "    * Are the parameters reaching a steady value or constantly changing?\n",
    "        * If steady value, smaller deviation in values\n",
    "        * If changing then there is data that doesn't get classified correctly\n",
    "\n",
    "## Example\n",
    "* Determine if a horse will die from colic\n",
    "    * Sample Data: 368 instances w/ 28 features\n",
    "* Data Problem: It's missing 30% of values\n",
    "\n",
    "* Options\n",
    "    * Use the feature's mean value from all available data\n",
    "    * Fill in the unknown with a special value like 0\n",
    "    * Ignore the instance\n",
    "    * Use a mean value from similar items\n",
    "    * Use another ML algorithm to predict the value\n",
    "\n",
    "## Summary\n",
    "* Logistic regression is about finding best-fit parameters to a sigmoid function\n",
    "* Optimization methods can be used to find these best-fit parameters\n",
    "* Gradient Ascent and Stochastic Gradient Ascent are the most common optimization methods\n",
    "* Stochastic Gradient Ascent has the benefit of being computationally cheap and is an online/steaming algorithm."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
