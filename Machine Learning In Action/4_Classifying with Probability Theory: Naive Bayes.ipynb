{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Classifying with Probability Theory: Naive Bayes\n",
    "* Let's start putting probabilities associated with our predictions\n",
    "\n",
    "## Classifying with Bayesian Decision Tree\n",
    "* Pros:\n",
    "    * Works with small amounts of data\n",
    "    * Handles multiple classes\n",
    "* Cons:\n",
    "    * Sensitive to input data preparation\n",
    "* Works with:\n",
    "    * Nominal Values\n",
    "\n",
    "* Imagine a dataset with two classes:\n",
    "    * Probabilities:\n",
    "        * P1(x, y) > P2(x, y) -> Class = 1\n",
    "        * P2(x, y) > P1(x, y) -> Class = 2\n",
    "    * Choose the decision with the highest probability\n",
    "\n",
    "## Classifying with Conditional Probabilities\n",
    "* P(c1|x, y) > P(c2|x,y): Class C1\n",
    "* P(c2|x,y) > P(c1|x,y): Class C2\n",
    "\n",
    "## Document Classification w/ Naive Bayes:\n",
    "* Features: Words in the document\n",
    "* General Approach:\n",
    "    * Collect: Any method\n",
    "    * Prepare: Numeric / Boolean values needed\n",
    "    * Analyze: Look at histograms as many features won't help indicate importance\n",
    "    * Train: Calculate the conditional probabilities of the independent features\n",
    "    * Test Calculate the error rate\n",
    "    * Use: Document Classification\n",
    "* 1000 features ends up being a solid number of examples\n",
    "* Assumptions:\n",
    "    * Every feature is independent:\n",
    "        * One feature is just as likely by itself as it is next to other words\n",
    "    * Every feature is equally important\n",
    "\n",
    "## Classifying text\n",
    "* Token: Any combination of characters\n",
    "* Goal: Transform the words into a cevtor\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
